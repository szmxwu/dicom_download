# -*- coding: utf-8 -*-
#!/usr/bin/env python3
"""
Flask Web åº”ç”¨ä¸»æ¨¡å—

æä¾› DICOM å¤„ç† Web æœåŠ¡å’Œ REST API æ¥å£ï¼Œæ”¯æŒï¼š
- PACS é…ç½®ç®¡ç†
- å•æ¡/æ‰¹é‡ä»»åŠ¡å¤„ç†
- æ–‡ä»¶ä¸Šä¼ å¤„ç†
- WebSocket å®æ—¶é€šä¿¡
"""

import os
import sys

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ï¼ˆç¡®ä¿èƒ½æ‰¾åˆ° src æ¨¡å—ï¼‰
# ä» src/web/app.py å‘ä¸Šä¸¤çº§åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from flask import Flask, render_template, request, jsonify, send_file
from flask_socketio import SocketIO, emit
import time
import uuid
import threading
import shutil
import logging
from logging.handlers import RotatingFileHandler
from werkzeug.utils import secure_filename

from dotenv import set_key
import secrets

# å¯¼å…¥æˆ‘ä»¬çš„DICOMå¤„ç†å®¢æˆ·ç«¯
from src.client.unified import DICOMDownloadClient
from src.utils.packaging import create_result_zip

def get_base_path():
    """è·å–ç¨‹åºè¿è¡Œæ—¶çš„æ ¹ç›®å½•è·¯å¾„ï¼Œå…¼å®¹ PyInstaller æ‰“åŒ…"""
    if hasattr(sys, '_MEIPASS'):
        return sys._MEIPASS
    return os.path.abspath(".")

def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼ˆä» src/web/ å‘ä¸Šä¸¤çº§ï¼‰"""
    current_file = os.path.abspath(__file__)
    # src/web/app.py -> å‘ä¸Šä¸¤çº§åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
    return os.path.dirname(os.path.dirname(os.path.dirname(current_file)))

# Flaskåº”ç”¨é…ç½® - æŒ‡å®šé™æ€æ–‡ä»¶å’Œæ¨¡æ¿è·¯å¾„ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾ï¼‰
project_root = get_project_root()
app = Flask(__name__,
            static_folder=os.path.join(project_root, 'static'),
            template_folder=os.path.join(project_root, 'templates'))

_secret_key = os.environ.get('FLASK_SECRET_KEY')
if not _secret_key:
    # ä»…ç”¨äºæœ¬åœ°/ä¸´æ—¶è¿è¡Œï¼›ç”Ÿäº§ç¯å¢ƒè¯·é€šè¿‡ç¯å¢ƒå˜é‡æä¾›å›ºå®šå€¼
    _secret_key = secrets.token_hex(32)
app.config['SECRET_KEY'] = _secret_key
app.config['UPLOAD_FOLDER'] = os.path.abspath('./uploads')
app.config['RESULT_FOLDER'] = os.path.abspath('./results')
app.config['MAX_CONTENT_LENGTH'] = 1500 * 1024 * 1024  # 1500MBæœ€å¤§æ–‡ä»¶å¤§å°

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['RESULT_FOLDER'], exist_ok=True)
os.makedirs('./temp', exist_ok=True)
os.makedirs('./logs', exist_ok=True)

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logging():
    logger = logging.getLogger('DICOMApp')
    logger.setLevel(logging.INFO)
    
    # é˜²æ­¢é‡å¤æ·»åŠ  handler
    if logger.handlers:
        return logger

    formatter = logging.Formatter(
        '[%(asctime)s] %(levelname)s in %(module)s: %(message)s'
    )

    # æ–‡ä»¶æ—¥å¿— (æŒ‰å¤§å°å›æ»š)
    file_handler = RotatingFileHandler(
        'logs/app.log', maxBytes=10*1024*1024, backupCount=5, encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    # æ§åˆ¶å°æ—¥å¿—
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

# WebSocketæ”¯æŒ
socketio = SocketIO(app, cors_allowed_origins="*")

# å…¨å±€å˜é‡å­˜å‚¨å¤„ç†ä»»åŠ¡
processing_tasks = {}

# åˆ›å»ºDICOMå®¢æˆ·ç«¯å®ä¾‹ç”¨äºç³»ç»ŸçŠ¶æ€æ£€æŸ¥ï¼ˆä¸ç™»å½•ï¼‰
dicom_client_checker = DICOMDownloadClient()

ENV_FILE_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')


def _parse_port(value, field_name: str) -> int:
    if value is None:
        raise ValueError(f"{field_name} is required")
    try:
        port = int(value)
    except (TypeError, ValueError):
        raise ValueError(f"{field_name} must be an integer")
    if port < 1 or port > 65535:
        raise ValueError(f"{field_name} must be between 1 and 65535")
    return port


def _normalize_aet(value, field_name: str) -> str:
    if value is None:
        raise ValueError(f"{field_name} is required")
    aet = str(value).strip()
    if not aet:
        raise ValueError(f"{field_name} is required")
    if len(aet) > 16:
        raise ValueError(f"{field_name} must be at most 16 characters")
    # DICOM AE Titles are typically uppercase ASCII
    aet = aet.upper()
    for ch in aet:
        if ord(ch) < 32 or ord(ch) > 126:
            raise ValueError(f"{field_name} contains invalid characters")
    return aet


def _normalize_host(value) -> str:
    if value is None:
        raise ValueError("PACS_IP is required")
    host = str(value).strip()
    if not host:
        raise ValueError("PACS_IP is required")
    if len(host) > 255:
        raise ValueError("PACS_IP is too long")
    return host

# è‡ªåŠ¨æ¸…ç†é…ç½®ï¼ˆä» .env è¯»å–ï¼Œè‹¥ä¸å­˜åœ¨åˆ™å†™å› .envï¼‰
try:
    CLEANUP_THRESHOLD_GB = float(os.getenv('CLEANUP_THRESHOLD_GB', '50'))
except Exception:
    CLEANUP_THRESHOLD_GB = 50.0

try:
    CLEANUP_TARGET_GB = float(os.getenv('CLEANUP_TARGET_GB', '40'))
except Exception:
    CLEANUP_TARGET_GB = 40.0

# å°†è¯»å–åˆ°çš„é»˜è®¤å€¼æŒä¹…åŒ–åˆ° .envï¼Œä¾¿äºç”¨æˆ·ä¿®æ”¹ä¸æŒä¹…åŒ–é…ç½®
try:
    # å†™å…¥æ•´æ•°å€¼æ—¶ä¿ç•™æ•´å‹æ ¼å¼ï¼Œæµ®ç‚¹æ•°ä¿ç•™åŸæ ·
    thr_val = str(int(CLEANUP_THRESHOLD_GB) if float(CLEANUP_THRESHOLD_GB).is_integer() else CLEANUP_THRESHOLD_GB)
    tgt_val = str(int(CLEANUP_TARGET_GB) if float(CLEANUP_TARGET_GB).is_integer() else CLEANUP_TARGET_GB)
    set_key(ENV_FILE_PATH, 'CLEANUP_THRESHOLD_GB', thr_val)
    set_key(ENV_FILE_PATH, 'CLEANUP_TARGET_GB', tgt_val)
except Exception as e:
    logger.warning(f"æ— æ³•å°†æ¸…ç†é˜ˆå€¼å†™å…¥ {ENV_FILE_PATH}: {e}")

def get_directory_size(directory):
    """è®¡ç®—ç›®å½•æ€»å¤§å°ï¼ˆGBï¼‰"""
    total_size = 0
    try:
        for dirpath, dirnames, filenames in os.walk(directory):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                if os.path.exists(filepath):
                    total_size += os.path.getsize(filepath)
    except (OSError, IOError) as e:
        logger.warning(f"è®¡ç®—ç›®å½•å¤§å°æ—¶å‡ºé”™: {str(e)}")
    return total_size / (1024 ** 3)  # è½¬æ¢ä¸ºGB

def cleanup_old_results():
    """æ¸…ç†æ—§çš„ç»“æœæ–‡ä»¶ï¼Œä¿æŒç£ç›˜ç©ºé—´åœ¨åˆç†èŒƒå›´å†…"""
    results_dir = app.config['RESULT_FOLDER']
    current_size = get_directory_size(results_dir)
    
    if current_size < CLEANUP_THRESHOLD_GB:
        return
    
    logger.info(f"ç»“æœç›®å½•å¤§å°: {current_size:.2f}GB, å¯åŠ¨è‡ªåŠ¨æ¸…ç†")
    
    # è·å–æ‰€æœ‰å­ç›®å½•ï¼ˆä»»åŠ¡ç›®å½•å’ŒZIPæ–‡ä»¶ï¼‰
    items_to_check = []
    
    try:
        # æ‰«ææ‰€æœ‰æ–‡ä»¶å’Œç›®å½•
        for item in os.listdir(results_dir):
            item_path = os.path.join(results_dir, item)
            if os.path.exists(item_path):
                # è·å–æœ€åè®¿é—®æ—¶é—´
                atime = os.path.getatime(item_path)
                size = 0
                
                if os.path.isfile(item_path):
                    size = os.path.getsize(item_path) / (1024 ** 3)
                elif os.path.isdir(item_path):
                    size = get_directory_size(item_path)
                    
                items_to_check.append({
                    'path': item_path,
                    'name': item,
                    'atime': atime,
                    'size': size,
                    'is_dir': os.path.isdir(item_path)
                })
                
    except Exception as e:
        logger.error(f"æ‰«æç»“æœç›®å½•å¤±è´¥: {str(e)}")
        return
    
    # æ’é™¤æ­£åœ¨è¿›è¡Œçš„ä»»åŠ¡
    active_task_ids = [task.task_id for task in processing_tasks.values() 
                      if task.status in ['running', 'pending']]
    
    # è¿‡æ»¤æ‰æ­£åœ¨è¿›è¡Œçš„ä»»åŠ¡
    items_to_clean = []
    for item in items_to_check:
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ´»è·ƒä»»åŠ¡ç›®å½•
        is_active = False
        for task_id in active_task_ids:
            if task_id in item['name']:
                is_active = True
                break
        
        if not is_active:
            items_to_clean.append(item)
    
    if not items_to_clean:
        logger.info("æ‰€æœ‰æ–‡ä»¶éƒ½å±äºæ´»è·ƒä»»åŠ¡ï¼Œè·³è¿‡æ¸…ç†")
        return
    
    # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼Œå…ˆåˆ é™¤æœ€æ—§çš„
    items_to_clean.sort(key=lambda x: x['atime'])
    
    cleaned_size = 0
    target_to_clean = current_size - CLEANUP_TARGET_GB
    
    for item in items_to_clean:
        if cleaned_size >= target_to_clean:
            break
            
        try:
            logger.info(f"åˆ é™¤: {item['name']} ({item['size']:.2f}GB)")
            
            if item['is_dir']:
                shutil.rmtree(item['path'])
            else:
                os.remove(item['path'])
                
            cleaned_size += item['size']
            
        except Exception as e:
            logger.error(f"åˆ é™¤ {item['name']} å¤±è´¥: {str(e)}")
    
    final_size = get_directory_size(results_dir)
    logger.info(f"æ¸…ç†å®Œæˆ: {current_size:.2f}GB â†’ {final_size:.2f}GB (æ¸…ç†äº† {cleaned_size:.2f}GB)")

def check_and_cleanup_results():
    """æ£€æŸ¥å¹¶æ¸…ç†ç»“æœç›®å½•çš„åå°ä»»åŠ¡"""
    def cleanup_thread():
        try:
            cleanup_old_results()
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ¸…ç†å¤±è´¥: {str(e)}")
    
    # å¼‚æ­¥æ‰§è¡Œæ¸…ç†ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
    threading.Thread(target=cleanup_thread, daemon=True).start()

class ProcessingTask:
    """å¤„ç†ä»»åŠ¡ç±» - ä¿®å¤ç‰ˆ"""
    def __init__(self, task_id, task_type, parameters):
        self.task_id = task_id
        self.task_type = task_type
        self.parameters = parameters
        self.status = 'pending'
        self.progress = 0
        self.current_step = ''
        self.steps = []
        self.result = None
        self.error = None
        self.start_time = time.time()
        self.end_time = None
        self.logs = []

    def add_log(self, message, level='info'):
        """æ·»åŠ æ—¥å¿—"""
        log_entry = {
            'timestamp': time.strftime('%H:%M:%S'),
            'level': level,
            'message': message
        }
        self.logs.append(log_entry)
        
        # ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿè®°å½•
        log_method = getattr(logger, level.lower(), logger.info)
        log_method(f"[Task {self.task_id}] {message}")
        
        # é€šè¿‡WebSocketå‘é€æ›´æ–°
        try:
            socketio.emit('task_update', {
                'task_id': self.task_id,
                'status': self.status,
                'progress': self.progress,
                'current_step': self.current_step,
                'logs': self.logs[-5:]  # åªå‘é€æœ€æ–°5æ¡æ—¥å¿—
            })  # é»˜è®¤å¹¿æ’­ç»™æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
        except Exception as e:
            logger.error(f"WebSocketå‘é€å¤±è´¥: {str(e)}")

    def update_status(self, status, progress=None, step=None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        self.status = status
        if progress is not None:
            self.progress = progress
        if step is not None:
            self.current_step = step
        
        # ä½¿ç”¨ logger è®°å½•çŠ¶æ€è½¬æ¢
        logger.info(f"Task {self.task_id} status update: {status} ({progress or 0}% - {step or 'N/A'})")
        
        # é€šè¿‡WebSocketå‘é€æ›´æ–°
        try:
            socketio.emit('task_update', {
                'task_id': self.task_id,
                'status': self.status,
                'progress': self.progress,
                'current_step': self.current_step,
                'logs': self.logs[-5:]  # åªå‘é€æœ€æ–°5æ¡æ—¥å¿—
            })  # é»˜è®¤å¹¿æ’­ç»™æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
        except Exception as e:
            logger.error(f"WebSocketå‘é€å¤±è´¥: {str(e)}")

@app.route('/api/debug/test-connection')
def test_connection():
    """æµ‹è¯•PACSè¿æ¥çš„è°ƒè¯•æ¥å£"""
    try:
        client = DICOMDownloadClient()
        status = client.check_status()
        
        return jsonify({
            'pacs_connected': status,
            'pacs_config': {
                'ip': client.pacs_config['PACS_IP'],
                'port': client.pacs_config['PACS_PORT'],
                'calling_aet': client.pacs_config['CALLING_AET'],
                'called_aet': client.pacs_config['CALLED_AET']
            },
            'message': 'PACS connection OK' if status else 'PACS connection failed'
        })
    except Exception as e:
        return jsonify({
            'error': str(e),
            'message': 'Error occurred during test connection'
        }), 500
# è·¯ç”±å®šä¹‰
@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return render_template('index.html')

@app.route('/api/process/single', methods=['POST'])
def process_single():
    """å¤„ç†å•ä¸ªAccessionNumber"""
    logger.debug(f"process_singleè¢«è°ƒç”¨ï¼ŒIP: {request.remote_addr}")
    try:
        data = request.json
        accession_number = data.get('accession_number')
        options = data.get('options', {})
        
        if not accession_number:
            return jsonify({'error': 'Please provide AccessionNumber'}), 400
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task = ProcessingTask(task_id, 'single', {
            'accession_number': accession_number,
            'options': options
        })
        
        processing_tasks[task_id] = task
        
        # å¯åŠ¨åå°å¤„ç†
        threading.Thread(target=process_single_task, args=(task,)).start()
        
        return jsonify({
            'task_id': task_id,
            'status': 'started',
            'message': f'å¼€å§‹å¤„ç†AccessionNumber: {accession_number}'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/process/batch', methods=['POST'])
def process_batch():
    """æ‰¹é‡å¤„ç†å¤šä¸ªAccessionNumber"""
    try:
        data = request.json
        accession_numbers = data.get('accession_numbers', [])
        options = data.get('options', {})
        
        if not accession_numbers:
            return jsonify({'error': 'Please provide AccessionNumber list'}), 400
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task = ProcessingTask(task_id, 'batch', {
            'accession_numbers': accession_numbers,
            'options': options
        })
        
        processing_tasks[task_id] = task
        
        # å¯åŠ¨åå°å¤„ç†
        threading.Thread(target=process_batch_task, args=(task,)).start()
        
        return jsonify({
            'task_id': task_id,
            'status': 'started',
            'message': f'å¼€å§‹æ‰¹é‡å¤„ç† {len(accession_numbers)} ä¸ªç ”ç©¶'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/process/upload', methods=['POST'])
def process_upload():
    """å¤„ç†ä¸Šä¼ çš„ZIPæ–‡ä»¶"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file uploaded'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        if not file.filename.lower().endswith('.zip'):
            return jsonify({'error': 'Only ZIP files are supported'}), 400
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        filename = secure_filename(file.filename)
        timestamp = int(time.time())
        filename = f"{timestamp}_{filename}"
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        # è·å–å¤„ç†é€‰é¡¹
        options = {}
        for key in request.form:
            options[key] = request.form[key] == 'true'
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task = ProcessingTask(task_id, 'upload', {
            'filepath': filepath,
            'filename': filename,
            'options': options
        })
        
        processing_tasks[task_id] = task
        
        # å¯åŠ¨åå°å¤„ç†
        threading.Thread(target=process_upload_task, args=(task,)).start()
        
        return jsonify({
            'task_id': task_id,
            'status': 'started',
            'message': f'å¼€å§‹å¤„ç†ä¸Šä¼ æ–‡ä»¶: {file.filename}'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/task/<task_id>/status')
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    task = processing_tasks.get(task_id)
    if not task:
        return jsonify({'error': 'Task not found'}), 404
    
    return jsonify({
        'task_id': task.task_id,
        'status': task.status,
        'progress': task.progress,
        'current_step': task.current_step,
        'steps': task.steps,
        'logs': task.logs,
        'result': task.result,
        'error': task.error,
        'duration': (task.end_time or time.time()) - task.start_time
    })


def _serialize_task_history(task: 'ProcessingTask') -> dict:
    result = task.result or {}
    duration = (task.end_time or time.time()) - task.start_time
    summary = ''

    if task.task_type == 'single':
        summary = task.parameters.get('accession_number', '')
    elif task.task_type == 'batch':
        count = len(task.parameters.get('accession_numbers', []) or [])
        summary = f"{count} studies"
    elif task.task_type == 'upload':
        summary = task.parameters.get('filename', '')

    return {
        'task_id': task.task_id,
        'task_type': task.task_type,
        'status': task.status,
        'summary': summary,
        'start_time': task.start_time,
        'end_time': task.end_time,
        'duration': duration,
        'has_excel': bool(result.get('excel_file')),
        'has_zip': bool(result.get('result_zip') or result.get('zip_file')),
        'series_count': len(result.get('series_info', {}) or {}) if isinstance(result.get('series_info'), dict) else result.get('series_count', 0)
    }


@app.route('/api/tasks/history')
def get_task_history():
    """è¿”å›å·²å®Œæˆä»»åŠ¡çš„å†å²åˆ—è¡¨"""
    completed_tasks = [t for t in processing_tasks.values() if t.status == 'completed']
    completed_tasks.sort(key=lambda x: x.end_time or x.start_time, reverse=True)
    return jsonify({
        'tasks': [_serialize_task_history(task) for task in completed_tasks]
    })

@app.route('/api/task/<task_id>/cancel', methods=['POST'])
def cancel_task(task_id):
    """å–æ¶ˆä»»åŠ¡"""
    task = processing_tasks.get(task_id)
    if not task:
        return jsonify({'error': 'Task not found'}), 404
    
    if task.status in ['completed', 'failed', 'cancelled']:
        return jsonify({'error': 'Task completed, cannot cancel'}), 400
    
    task.update_status('cancelled')
    task.add_log('ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ', 'warning')
    
    return jsonify({'message': 'Task cancelled'})

@app.route('/api/download/<task_id>/<file_type>')
def download_result(task_id, file_type):
    """ä¸‹è½½å¤„ç†ç»“æœ"""
    task = processing_tasks.get(task_id)
    if not task or not task.result:
        logger.warning(f"ä¸‹è½½è¯·æ±‚å¤±è´¥: ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— ç»“æœ - task_id={task_id}")
        return jsonify({'error': 'Result file not found'}), 404
    
    try:
        if file_type == 'excel' and 'excel_file' in task.result:
            file_path = task.result['excel_file']
            logger.info(f"ä¸‹è½½Excelæ–‡ä»¶: {file_path}")
            if not file_path or not os.path.exists(file_path):
                logger.error(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return jsonify({'error': f'File not found: {file_path}'}), 404
            
            # Windowsä¸‹ä½¿ç”¨pathlibå¤„ç†è·¯å¾„
            from pathlib import Path
            file_path = str(Path(file_path).resolve())
            
            return send_file(
                file_path,
                as_attachment=True,
                download_name=f"metadata_{task_id}.xlsx",
                mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
        elif (file_type == 'zip' or file_type == 'result_zip') and (task.result.get('zip_file') or task.result.get('result_zip')):
            path = task.result.get('result_zip') or task.result.get('zip_file')
            logger.info(f"ä¸‹è½½ZIPæ–‡ä»¶: path={path}, exists={os.path.exists(path) if path else False}")
            
            if not path:
                logger.error("ZIPè·¯å¾„ä¸ºç©º")
                return jsonify({'error': 'ZIP path is empty'}), 404
            
            # Windowsä¸‹ä½¿ç”¨pathlibå¤„ç†è·¯å¾„
            from pathlib import Path
            abs_path = str(Path(path).resolve())
            logger.info(f"ç»å¯¹è·¯å¾„: {abs_path}, exists={os.path.exists(abs_path)}")
            
            if not os.path.exists(abs_path):
                logger.error(f"ZIPæ–‡ä»¶ä¸å­˜åœ¨: {abs_path}")
                # å°è¯•åˆ—å‡ºresultsç›®å½•å†…å®¹å¸®åŠ©è°ƒè¯•
                try:
                    result_dir = app.config['RESULT_FOLDER']
                    files = os.listdir(result_dir)
                    logger.info(f"Resultsç›®å½•å†…å®¹: {files}")
                except Exception as list_e:
                    logger.error(f"æ— æ³•åˆ—å‡ºresultsç›®å½•: {list_e}")
                return jsonify({'error': f'ZIP file not found: {abs_path}'}), 404
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            try:
                file_size = os.path.getsize(abs_path)
                logger.info(f"ZIPæ–‡ä»¶å¤§å°: {file_size} bytes")
                if file_size == 0:
                    logger.error("ZIPæ–‡ä»¶å¤§å°ä¸º0")
                    return jsonify({'error': 'ZIP file is empty'}), 500
            except Exception as size_e:
                logger.error(f"æ— æ³•è·å–æ–‡ä»¶å¤§å°: {size_e}")
            
            logger.info(f"å¼€å§‹å‘é€æ–‡ä»¶: {abs_path}")
            response = send_file(
                abs_path,
                as_attachment=True,
                download_name=f"organized_{task_id}.zip",
                mimetype='application/zip'
            )
            logger.info(f"æ–‡ä»¶å‘é€æˆåŠŸ: {abs_path}")
            return response
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}, result keys={list(task.result.keys())}")
            return jsonify({'error': 'File type not supported'}), 400
            
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {type(e).__name__}: {e}", exc_info=True)
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'{type(e).__name__}: {str(e)}'}), 500

@app.route('/api/system/status')
def system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    # æ£€æŸ¥DICOMæœåŠ¡çŠ¶æ€
    dicom_status = 'unknown'
    error_msg = None
    try:
        # æ¯æ¬¡è°ƒç”¨åˆ›å»ºç‹¬ç«‹çš„å®¢æˆ·ç«¯ï¼Œé¿å…æŒä¹…è¿æ¥çŠ¶æ€å¼‚å¸¸
        client = DICOMDownloadClient()
        if client.check_status():
            dicom_status = 'connected'
        else:
            dicom_status = 'disconnected'
    except Exception as e:
        dicom_status = 'error'
        error_msg = str(e)
        logger.error(f"DICOMçŠ¶æ€æ£€æŸ¥å¤±è´¥: {error_msg}")
    
    # è·å–å­˜å‚¨ç©ºé—´ä¿¡æ¯
    results_size_gb = get_directory_size(app.config['RESULT_FOLDER'])
    
    response = {
        'status': 'running',
        'active_tasks': len([t for t in processing_tasks.values() if t.status == 'running']),
        'total_tasks': len(processing_tasks),
        'dicom_service_status': dicom_status,
        'storage': {
            'results_size_gb': round(results_size_gb, 2),
            'cleanup_threshold_gb': CLEANUP_THRESHOLD_GB,
            'cleanup_needed': results_size_gb >= CLEANUP_THRESHOLD_GB
        }
    }
    
    if error_msg:
        response['dicom_error'] = error_msg
    
    return jsonify(response)


@app.route('/api/pacs-config', methods=['GET'])
def get_pacs_config():
    """Get current PACS configuration (from environment/defaults)."""
    defaults = {
        'PACS_IP': '172.17.250.192',
        'PACS_PORT': 2104,
        'CALLING_AET': 'WMX01',
        'CALLED_AET': 'pacsFIR',
        'CALLING_PORT': 1103,
    }

    def _get_env_int(key, default):
        try:
            return int(os.getenv(key, default))
        except (TypeError, ValueError):
            return default

    return jsonify({
        'PACS_IP': os.getenv('PACS_IP', defaults['PACS_IP']),
        'PACS_PORT': _get_env_int('PACS_PORT', defaults['PACS_PORT']),
        'CALLING_AET': os.getenv('CALLING_AET', defaults['CALLING_AET']),
        'CALLED_AET': os.getenv('CALLED_AET', defaults['CALLED_AET']),
        'CALLING_PORT': _get_env_int('CALLING_PORT', defaults['CALLING_PORT']),
    })


@app.route('/api/pacs-config', methods=['POST'])
def set_pacs_config():
    """Persist PACS configuration to .env and update process env for new connections."""
    global dicom_client_checker

    data = request.json or {}
    try:
        pacs_ip = _normalize_host(data.get('PACS_IP'))
        pacs_port = _parse_port(data.get('PACS_PORT'), 'PACS_PORT')
        calling_aet = _normalize_aet(data.get('CALLING_AET'), 'CALLING_AET')
        called_aet = _normalize_aet(data.get('CALLED_AET'), 'CALLING_AET')
        calling_port = _parse_port(data.get('CALLING_PORT'), 'CALLING_PORT')
    except ValueError as e:
        return jsonify({'error': str(e)}), 400

    try:
        # Write to .env
        os.makedirs(os.path.dirname(ENV_FILE_PATH), exist_ok=True)
        set_key(ENV_FILE_PATH, 'PACS_IP', pacs_ip)
        set_key(ENV_FILE_PATH, 'PACS_PORT', str(pacs_port))
        set_key(ENV_FILE_PATH, 'CALLING_AET', calling_aet)
        set_key(ENV_FILE_PATH, 'CALLED_AET', called_aet)
        set_key(ENV_FILE_PATH, 'CALLING_PORT', str(calling_port))

        # Update in-memory env so new client instances pick it up without restart
        os.environ['PACS_IP'] = pacs_ip
        os.environ['PACS_PORT'] = str(pacs_port)
        os.environ['CALLING_AET'] = calling_aet
        os.environ['CALLED_AET'] = called_aet
        os.environ['CALLING_PORT'] = str(calling_port)

        # Refresh global checker client
        dicom_client_checker = DICOMDownloadClient()

        return jsonify({'message': 'Configuration saved'}), 200
    except Exception as e:
        return jsonify({'error': f'Failed to save configuration: {str(e)}'}), 500

# å¤„ç†ä»»åŠ¡å‡½æ•°
def process_single_task(task):
    """å¤„ç†å•ä¸ªAccessionNumberä»»åŠ¡ - ä¿®å¤ç‰ˆ"""
    client_logged_in = False
    task_client = None
    
    try:
        # ç«‹å³æ›´æ–°çŠ¶æ€ï¼Œç¡®ä¿WebSocketå‘é€
        task.update_status('running', 5, 'Connecting to DICOM service')
        task.add_log("Connecting to DICOM service...")
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logger.debug(f"å¼€å§‹å¤„ç†ä»»åŠ¡: {task.task_id}")
        logger.debug(f"AccessionNumber: {task.parameters['accession_number']}")
        
        # åˆ›å»ºæ–°çš„DICOMå®¢æˆ·ç«¯å®ä¾‹
        try:
            task_client = DICOMDownloadClient()
            task.add_log("DICOM client created successfully")
        except Exception as e:
            task.add_log(f"Failed to create DICOM client: {str(e)}", 'error')
            raise Exception(f"Failed to create DICOM client: {str(e)}")
        
        # æ£€æŸ¥PACSè¿æ¥çŠ¶æ€
        task.update_status('running', 8, 'Checking PACS connection')
        task.add_log("Checking PACS connection status...")
        
        try:
            if not task_client.check_status():
                raise Exception("PACS service unavailable, please check network and configuration")
            task.add_log("PACS connection normal")
        except Exception as e:
            task.add_log(f"PACS connection check failed: {str(e)}", 'error')
            raise
        
        # è‡ªåŠ¨ç™»å½•ï¼ˆå…¼å®¹æ€§æ¥å£ï¼šå½“å‰DICOMå®¢æˆ·ç«¯ä¸åšçœŸå®è®¤è¯ï¼‰
        task.update_status('running', 10, 'Logging in to DICOM service')
        task.add_log("Logging in to DICOM service...")
        
        try:
            username = os.getenv('DICOM_USERNAME', '')
            password = os.getenv('DICOM_PASSWORD', '')
            if not task_client.login(username, password):
                raise Exception("DICOM service login failed")
            client_logged_in = True
            task.add_log("DICOM service login successful")
        except Exception as e:
            task.add_log(f"Login failed: {str(e)}", 'error')
            raise

        # Attach progress callback so MR_clean can report progress back to task logs
        def _mr_progress(msg, stage=None):
            try:
                task.add_log(msg)
            except Exception:
                pass
            logger.info(f"MR_PROGRESS[{stage}]: {msg}")

        task_client.progress_callback = _mr_progress
        
        # Attach download progress callback to update task status during C-MOVE
        def _download_progress(current_series, total_series, series_name, progress_pct):
            try:
                step_name = f"Downloading series {current_series}/{total_series}: {series_name}"
                task.update_status('running', progress_pct, step_name)
                if current_series % 3 == 0 or current_series == total_series:  # æ¯3ä¸ªseriesè®°å½•ä¸€æ¬¡æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    task.add_log(f"Downloaded {current_series}/{total_series} series: {series_name}")
            except Exception as e:
                logger.warning(f"Download progress callback error: {e}")
        
        task_client.download_progress_callback = _download_progress
        
        # è·å–å‚æ•°
        accession_number = task.parameters['accession_number']
        options = task.parameters.get('options', {})
        
        task.update_status('running', 15, 'Preparing process')
        task.add_log(f"Start processing AccessionNumber: {accession_number}")
        
        # åˆ›å»ºç»“æœç›®å½•
        result_dir = os.path.join(app.config['RESULT_FOLDER'], task.task_id)
        os.makedirs(result_dir, exist_ok=True)
        task.add_log(f"Created result directory: {result_dir}")
        
        # è®¾ç½®å¤„ç†æ­¥éª¤
        task.steps = ['Connect Service', 'Query Data', 'Download Files', 'Organize Files', 'NIfTI Conversion', 'Extract Metadata']
        
        # æ‰§è¡Œå¤„ç†æµç¨‹
        task.update_status('running', 30, 'Querying PACS data')
        task.add_log('Querying data from PACS...')
        
        # å…ˆæŸ¥è¯¢æ˜¯å¦å­˜åœ¨æ•°æ®
        try:
            series_metadata = task_client._query_series_metadata(accession_number)
            if not series_metadata:
                raise Exception(f"AccessionNumber not found in PACS: {accession_number}")
            task.add_log(f"Found {len(series_metadata)} Series")
        except Exception as e:
            task.add_log(f"Query failed: {str(e)}", 'error')
            raise
        
        task.update_status('running', 40, 'Downloading DICOM files')
        task.add_log('Downloading DICOM files...')
        
        try:
            # ä½¿ç”¨å·²ç™»å½•çš„å®¢æˆ·ç«¯å¤„ç†
            results = task_client.process_complete_workflow(
                accession_number=accession_number,
                base_output_dir=result_dir,
                auto_extract=options.get('auto_extract', True),
                auto_organize=options.get('auto_organize', True),
                auto_metadata=options.get('auto_metadata', True),
                keep_zip=options.get('keep_zip', True),
                keep_extracted=options.get('keep_extracted', False),
                output_format=options.get('output_format', 'nifti'),
                parallel_pipeline=False  # ç¦ç”¨å¹¶è¡Œæµæ°´çº¿ï¼Œä½¿ç”¨å•çº¿ç¨‹é¡ºåºå¤„ç†
            )
            
            if results and results.get('success'):
                task.update_status('running', 90, 'Generating results')
                task.add_log('Process successful, generating results...')
                
                # åˆ›å»ºç»“æœZIPæ–‡ä»¶
                if results.get('organized_dir'):
                    task.add_log('Creating result ZIP...')
                    logger.info(f"Creating ZIP: source={results['organized_dir']}, task_id={task.task_id}, result_dir={app.config['RESULT_FOLDER']}")
                    try:
                        zip_path = create_result_zip(
                            results['organized_dir'],
                            task.task_id,
                            app.config['RESULT_FOLDER'],
                            extra_files=[results.get('excel_file')]
                        )
                        results['result_zip'] = zip_path
                        task.add_log(f'Result ZIP created: {zip_path}')
                        logger.info(f"ZIP created successfully: {zip_path}, exists={os.path.exists(zip_path)}")
                    except Exception as zip_e:
                        logger.error(f"Failed to create ZIP: {zip_e}", exc_info=True)
                        task.add_log(f'Failed to create ZIP: {str(zip_e)}', 'error')
                        raise
                
                task.result = results
                task.update_status('completed', 100, 'Completed')
                task.add_log('âœ… Process completed successfully!', 'success')
                task.end_time = time.time()
                
                # è¾“å‡ºæˆåŠŸæ—¥å¿—
                logger.debug(f"ä»»åŠ¡å®Œæˆ: {task.task_id}")
                
                # ä»»åŠ¡å®Œæˆåæ£€æŸ¥å¹¶æ¸…ç†ç»“æœç›®å½•
                check_and_cleanup_results()
                
            else:
                error_msg = results.get('error', 'Unknown error during process') if results else 'No result returned'
                task.add_log(f'Process failed: {error_msg}', 'error')
                task.update_status('failed')
                task.error = error_msg
                
        except Exception as e:
            task.add_log(f'Error during process: {str(e)}', 'error')
            raise
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"ä»»åŠ¡å¤„ç†å¤±è´¥: {task.task_id}, é”™è¯¯: {error_msg}")
        task.add_log(f'Process error: {error_msg}', 'error')
        task.update_status('failed')
        task.error = error_msg
        task.end_time = time.time()
    
    finally:
        # ç¡®ä¿ç™»å‡º
        if client_logged_in and task_client:
            try:
                task_client.logout()
                task.add_log("Logged out from DICOM service")
                logger.debug(f"å·²ç™»å‡ºDICOMæœåŠ¡")
            except Exception as e:
                task.add_log(f"Error during logout: {str(e)}", 'warning')
                logger.warning(f"ç™»å‡ºå¤±è´¥: {str(e)}")

def process_batch_task(task):
    """å¤„ç†æ‰¹é‡AccessionNumberä»»åŠ¡"""
    client_logged_in = False
    task_client = None  # åˆå§‹åŒ–å˜é‡
    try:
        accession_numbers = task.parameters['accession_numbers']
        options = task.parameters['options']
        
        task.update_status('running', 5, 'Connecting to DICOM service')
        task.add_log("Connecting to DICOM service...")
        
        # åˆ›å»ºæ–°çš„DICOMå®¢æˆ·ç«¯å®ä¾‹å¹¶ç™»å½•
        task_client = DICOMDownloadClient()
        
        # è‡ªåŠ¨ç™»å½•ï¼ˆå…¼å®¹æ€§æ¥å£ï¼šå½“å‰DICOMå®¢æˆ·ç«¯ä¸åšçœŸå®è®¤è¯ï¼‰
        task.add_log("Logging in to DICOM service...")
        username = os.getenv('DICOM_USERNAME', '')
        password = os.getenv('DICOM_PASSWORD', '')
        if not task_client.login(username, password):
            raise Exception("DICOM service login failed, please check service status")
        
        client_logged_in = True
        task.add_log("DICOM service login successful")
        # Attach progress callback so MR_clean can report progress back to task logs
        def _mr_progress(msg, stage=None):
            try:
                task.add_log(msg)
            except Exception:
                pass
            logger.info(f"MR_PROGRESS[{stage}]: {msg}")

        task_client.progress_callback = _mr_progress
        
        task.update_status('running', 10, 'Initializing batch process')
        task.add_log(f"Start batch processing {len(accession_numbers)} studies")
        
        results = []
        total = len(accession_numbers)
        
        for i, accno in enumerate(accession_numbers):
            # è®¡ç®—è¿›åº¦ (10-90%ç”¨äºå¤„ç†ï¼Œå‰©ä½™ç”¨äºæ•´ç†)
            progress = 10 + int((i / total) * 80)
            task.update_status('running', progress, f'Processing {accno} ({i+1}/{total})')
            task.add_log(f'Processing study {i+1}/{total}: {accno}')
            
            # åˆ›å»ºå•ç‹¬çš„ç»“æœç›®å½•
            result_dir = os.path.join(app.config['RESULT_FOLDER'], task.task_id, accno)
            os.makedirs(result_dir, exist_ok=True)
            
            try:
                result = task_client.process_complete_workflow(
                    accession_number=accno,
                    base_output_dir=result_dir,
                    auto_extract=options.get('auto_extract', True),
                    auto_organize=options.get('auto_organize', True),
                    auto_metadata=options.get('auto_metadata', True),
                    output_format=options.get('output_format', 'nifti'),
                    parallel_pipeline=False  # ç¦ç”¨å¹¶è¡Œæµæ°´çº¿ï¼Œä½¿ç”¨å•çº¿ç¨‹é¡ºåºå¤„ç†
                )
                results.append(result)
                task.add_log(f'{accno} Process completed')
                
            except Exception as e:
                task.add_log(f'{accno} Process failed: {str(e)}', 'error')
                results.append({'accession_number': accno, 'error': str(e)})
        
        task.update_status('running', 95, 'Creating batch results')
        task.add_log("Creating batch result files...")
        
        # åˆå¹¶ç»“æœ
        batch_result_dir = os.path.join(app.config['RESULT_FOLDER'], task.task_id)
        zip_path = create_result_zip(
            batch_result_dir,
            f"batch_{task.task_id}",
            app.config['RESULT_FOLDER']
        )
        
        task.result = {
            'batch_results': results,
            'result_zip': zip_path,
            'total_processed': len([r for r in results if r.get('success')]),
            'total_failed': len([r for r in results if r.get('error')])
        }
        
        task.update_status('completed', 100, 'Batch process completed')
        task.add_log('Batch process completed')
        task.end_time = time.time()
        
        # æ‰¹é‡ä»»åŠ¡å®Œæˆåæ£€æŸ¥å¹¶æ¸…ç†ç»“æœç›®å½•
        check_and_cleanup_results()
        
    except Exception as e:
        task.add_log(f'Batch process error: {str(e)}', 'error')
        task.update_status('failed')
        task.error = str(e)
        task.end_time = time.time()
    
    finally:
        # ç¡®ä¿ç™»å‡º
        if client_logged_in and task_client:
            try:
                task_client.logout()
                task.add_log("Logged out from DICOM service")
            except Exception as e:
                task.add_log(f"Error during logout: {str(e)}", 'warning')

def process_upload_task(task):
    """å¤„ç†ä¸Šä¼ æ–‡ä»¶ä»»åŠ¡"""
    try:
        filepath = task.parameters['filepath']
        options = task.parameters['options']
        
        task.update_status('running', 5, 'Initializing upload process')
        task.add_log(f"Start processing uploaded file: {task.parameters['filename']}")
        
        # åˆ›å»ºæœ¬åœ°DICOMå®¢æˆ·ç«¯å®ä¾‹ï¼ˆæ— éœ€ç™»å½•ï¼Œä»…ç”¨äºæœ¬åœ°æ–‡ä»¶å¤„ç†ï¼‰
        local_client = DICOMDownloadClient()
        
        # åˆ›å»ºç»“æœç›®å½•
        result_dir = os.path.join(app.config['RESULT_FOLDER'], task.task_id)
        os.makedirs(result_dir, exist_ok=True)
        
        task.steps = ['Extract Files', 'Organize Files', 'NIfTI Conversion', 'Extract Metadata']

        # å¤„ç†ä¸Šä¼ æµç¨‹
        task.update_status('running', 20, 'Processing upload workflow')
        task.add_log("Processing uploaded ZIP file...")
        result = local_client.process_upload_workflow(filepath, result_dir, options)

        if result.get('success'):
            organized_dir = result.get('organized_dir')
            series_info = result.get('series_info', {})
            excel_file = result.get('excel_file')

            task.update_status('running', 95, 'Creating result files')
            task.add_log("Creating result files...")
            zip_path = create_result_zip(
                result_dir,
                task.task_id,
                app.config['RESULT_FOLDER']
            )

            task.result = {
                'extract_dir': result.get('extract_dir'),
                'organized_dir': organized_dir,
                'excel_file': excel_file,
                'result_zip': zip_path,
                'series_count': len(series_info)
            }

            task.update_status('completed', 100, 'Upload process completed')
            task.add_log('Upload process completed')
            task.end_time = time.time()

            # ä¸Šä¼ æ–‡ä»¶å¤„ç†å®Œæˆåæ£€æŸ¥å¹¶æ¸…ç†ç»“æœç›®å½•
            check_and_cleanup_results()
        else:
            task.add_log('Upload processing failed', 'error')
            task.update_status('failed')
            task.error = result.get('error') or 'Failed to process uploaded file'
            
    except Exception as e:
        task.add_log(f'Upload process error: {str(e)}', 'error')
        task.update_status('failed')
        task.error = str(e)
        task.end_time = time.time()


# WebSocketäº‹ä»¶å¤„ç†
@socketio.on('connect')
def handle_connect():
    logger.info('å®¢æˆ·ç«¯å·²è¿æ¥')

@socketio.on('disconnect')
def handle_disconnect():
    logger.info('å®¢æˆ·ç«¯å·²æ–­å¼€')

@socketio.on('subscribe_task')
def handle_subscribe_task(data):
    """è®¢é˜…ä»»åŠ¡æ›´æ–°"""
    task_id = data.get('task_id')
    if task_id in processing_tasks:
        emit('task_subscribed', {'task_id': task_id})

if __name__ == '__main__':
    logger.info("="*60)
    logger.info("ğŸ¥ DICOMå¤„ç†Webåº”ç”¨å¯åŠ¨")
    logger.info("="*60)
    logger.info("ğŸ“¡ è®¿é—®åœ°å€: http://172.17.250.136:5005")
    
    # æ£€æŸ¥DICOMæœåŠ¡è¿æ¥çŠ¶æ€
    try:
        checker = DICOMDownloadClient()
        if checker.check_status():
            logger.info("âœ… PACSæœåŠ¡è¿æ¥æ­£å¸¸")
            logger.info(f"   - PACS IP: {checker.pacs_config['PACS_IP']}")
            logger.info(f"   - PACS Port: {checker.pacs_config['PACS_PORT']}")
            logger.info(f"   - Calling AET: {checker.pacs_config['CALLING_AET']}")
            logger.info(f"   - Called AET: {checker.pacs_config['CALLED_AET']}")
        else:
            logger.warning("âš ï¸  PACSæœåŠ¡è¿æ¥å¼‚å¸¸ï¼Œä¸‹è½½åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    except Exception as e:
        logger.error(f"âš ï¸  æ— æ³•è¿æ¥PACSæœåŠ¡: {str(e)}")
        logger.info("   ä»…æ”¯æŒæœ¬åœ°æ–‡ä»¶ä¸Šä¼ å¤„ç†")
    
    logger.info("="*60)
    if os.getenv('DICOM_USERNAME') or os.getenv('DICOM_PASSWORD'):
        logger.info("ğŸ” å·²ä»ç¯å¢ƒå˜é‡è¯»å–DICOMç™»å½•ä¿¡æ¯")
    else:
        logger.info("ğŸ” æœªé…ç½®DICOMç™»å½•ä¿¡æ¯ï¼ˆå½“å‰å®ç°æ— éœ€çœŸå®è®¤è¯ï¼‰")
    logger.info("ğŸš€ ç³»ç»Ÿå·²å°±ç»ªï¼Œç­‰å¾…ç”¨æˆ·è¯·æ±‚...")
    logger.info("ğŸ“¡ æµ‹è¯•URL:")
    logger.info("   - http://localhost:5005")
    logger.info("   - http://127.0.0.1:5005") 
    logger.info("   - http://172.17.250.136:5005")
    logger.info("="*60)
    
    # å¯åŠ¨åº”ç”¨ï¼Œå¼€å¯è°ƒè¯•æ¨¡å¼å’Œè‡ªåŠ¨é‡è½½
    socketio.run(app, host='0.0.0.0', port=5005, debug=False, allow_unsafe_werkzeug=True)
