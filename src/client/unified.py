# -*- coding: utf-8 -*-
#!/usr/bin/env python3
"""
DICOM å®¢æˆ·ç«¯ç»Ÿä¸€æ¨¡å—

æä¾›ä¸ PACS æœåŠ¡å™¨é€šä¿¡ã€DICOM ä¸‹è½½ã€å¤„ç†æµç¨‹ç¼–æ’ç­‰åŠŸèƒ½ã€‚
æ”¯æŒ PACS æŸ¥è¯¢ä¸‹è½½æµç¨‹å’Œæœ¬åœ°ä¸Šä¼ å¤„ç†æµç¨‹ã€‚
"""

import os
import json
import time
import threading
import zipfile
from queue import Queue
import pandas as pd
import pydicom
import re
import sys
import logging
from types import SimpleNamespace
from src.core.metadata import extract_dicom_metadata as extract_dicom_metadata_impl
from src.core.organize import organize_dicom_files as organize_dicom_files_impl
from src.core.organize import process_single_series as process_single_series_impl
from src.core.convert import convert_dicom_to_nifti as convert_dicom_to_nifti_impl
from src.core.convert import convert_to_npz as convert_to_npz_impl
from src.core.convert import normalize_and_save_npz as normalize_and_save_npz_impl
from src.core.convert import convert_with_dcm2niix as convert_with_dcm2niix_impl
from src.core.convert import convert_with_python_libs as convert_with_python_libs_impl
from src.core.convert import apply_rescale as apply_rescale_impl
from src.core.convert import apply_photometric as apply_photometric_impl
from src.core.convert import build_affine_from_dicom as build_affine_from_dicom_impl
from src.core.preview import get_window_params as get_window_params_impl
from src.core.preview import apply_windowing as apply_windowing_impl
from src.core.preview import resize_with_aspect as resize_with_aspect_impl
from src.core.preview import normalize_2d_preview as normalize_2d_preview_impl
from src.core.preview import generate_series_preview as generate_series_preview_impl
from src.core.qc import assess_image_quality as assess_image_quality_impl
from src.core.qc import assess_image_quality_from_array as assess_image_quality_from_array_impl
from src.core.qc import assess_converted_file_quality as assess_converted_file_quality_impl
from src.core.qc import assess_series_quality_converted as assess_series_quality_converted_impl
from src.core.qc import assess_series_quality as assess_series_quality_impl
from pynetdicom import AE, evt, AllStoragePresentationContexts
from pynetdicom.sop_class import (
    StudyRootQueryRetrieveInformationModelFind,
    StudyRootQueryRetrieveInformationModelMove
)
from pydicom.dataset import Dataset

logger = logging.getLogger('DICOMApp')

def get_base_path():
    """è·å–ç¨‹åºè¿è¡Œæ—¶çš„æ ¹ç›®å½•è·¯å¾„ï¼Œå…¼å®¹ PyInstaller æ‰“åŒ…"""
    if hasattr(sys, '_MEIPASS'):
        return sys._MEIPASS
    return os.path.abspath(".")

class DICOMDownloadClient:
    """ç»Ÿä¸€ç‰ˆDICOMä¸‹è½½å®¢æˆ·ç«¯ï¼Œç›´æ¥ä¸PACSé€šä¿¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        # PACSé…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡åŠ è½½ï¼Œæä¾›é»˜è®¤å€¼ï¼‰
        self.pacs_config = {
            'PACS_IP': os.getenv('PACS_IP', '172.17.250.192'),
            'PACS_PORT': int(os.getenv('PACS_PORT', 2104)),
            'CALLING_AET': os.getenv('CALLING_AET', 'WMX01'),
            'CALLED_AET': os.getenv('CALLED_AET', 'pacsFIR'),
            'CALLING_PORT': int(os.getenv('CALLING_PORT', 1103))
        }
        
        # åˆå§‹åŒ–AE
        self.ae = AE(ae_title=self.pacs_config['CALLING_AET'])
        self.ae.add_requested_context(StudyRootQueryRetrieveInformationModelFind)
        self.ae.add_requested_context(StudyRootQueryRetrieveInformationModelMove)
        self.ae.network_timeout = 300
        self.ae.acse_timeout = 30
        self.ae.dimse_timeout = 300
        
        # åŠ è½½DICOMå­—æ®µåˆ—è¡¨
        self.modality_keywords = self._load_keywords()
        
        # å…¼å®¹æ€§å±æ€§
        self.session_id = "dummy_session"
        self.username = os.getenv('DICOM_USERNAME', '')
        self.role = os.getenv('DICOM_ROLE', 'admin')
        # optional progress callback to report MR_clean progress: function(message, stage)
        self.progress_callback = None
        # optional download progress callback: function(current_series, total_series, series_name)
        self.download_progress_callback = None
        # disk watermarks (GB) for download throttling
        try:
            self._download_high_watermark_gb = float(os.getenv('DOWNLOAD_HIGH_WATERMARK_GB', '45'))
            self._download_low_watermark_gb = float(os.getenv('DOWNLOAD_LOW_WATERMARK_GB', '40'))
        except Exception:
            self._download_high_watermark_gb = 45.0
            self._download_low_watermark_gb = 40.0
    
    def _load_keywords(self, tags_dir="dicom_tags"):
        """åŠ è½½ä¸åŒæ¨¡æ€çš„DICOMå­—æ®µåˆ—è¡¨"""
        keywords_map = {}
        default_keywords = [
            "Modality", "StudyDate", "StudyInstanceUID", "SeriesInstanceUID",
            "PatientID", "AccessionNumber", "SeriesNumber", "SeriesDescription",
            "BodyPartExamined", "Manufacturer", "ManufacturerModelName"
        ]
        
        try:
            if not os.path.exists(tags_dir):
                # å°è¯•ä½¿ç”¨æ—§çš„keywords.jsonä½œä¸ºé»˜è®¤
                keywords_path = os.path.join(get_base_path(), "keywords.json")
                if os.path.exists(keywords_path):
                    with open(keywords_path, 'r', encoding='utf-8') as f:
                        keywords_map['DEFAULT'] = json.load(f)
                    print(f"âš ï¸  {tags_dir} not found, using keywords.json as default")
                else:
                    print(f"âš ï¸  {tags_dir} not found, using built-in default keywords")
                return {'default': default_keywords}

            # åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶
            for filename in os.listdir(tags_dir):
                if filename.endswith('.json'):
                    modality = filename.replace('.json', '').upper()
                    try:
                        with open(os.path.join(tags_dir, filename), 'r', encoding='utf-8') as f:
                            keywords_map[modality] = json.load(f)
                        print(f"âœ… Loaded {modality} modality keywords ({len(keywords_map[modality])} items)")
                    except Exception as e:
                        print(f"âŒ Failed to load {filename}: {e}")
            
            # ç¡®ä¿æœ‰é»˜è®¤å€¼
            if 'MR' in keywords_map:
                keywords_map['default'] = keywords_map['MR']
            elif 'DEFAULT' in keywords_map:
                keywords_map['default'] = keywords_map['DEFAULT']
            else:
                keywords_map['default'] = default_keywords
                
            return keywords_map
            
        except Exception as e:
            print(f"âŒ Failed to load keywords files: {e}")
            return {'default': default_keywords}
    
    def get_keywords(self, modality):
        """æ ¹æ®æ¨¡æ€è·å–å­—æ®µåˆ—è¡¨"""
        # å½’ä¸€åŒ–æ¨¡æ€åç§°
        modality = modality.upper()
        if modality in ['DR', 'DX', 'CR']:
            key = 'DX'
        elif "MR" in modality:
            key = 'MR'
        elif modality in self.modality_keywords:
            key = modality
        else:
            key = 'default'
            
        return self.modality_keywords.get(key, self.modality_keywords.get('default', []))

    def login(self, username, password):
        """ä¿æŒæ¥å£å…¼å®¹æ€§çš„è™šæ‹Ÿç™»å½•"""
        self.username = username
        print(f"âœ… Login successful: {username} (no actual authentication required)")
        return True

    def _get_dir_size_gb(self, directory):
        """è®¡ç®—ç›®å½•å¤§å°ï¼ˆGBï¼‰ï¼Œç”¨äºç£ç›˜æ°´ä½åˆ¤æ–­ã€‚"""
        total_size = 0
        try:
            for dirpath, dirnames, filenames in os.walk(directory):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        try:
                            total_size += os.path.getsize(filepath)
                        except Exception:
                            continue
        except Exception:
            return 0.0
        return total_size / (1024 ** 3)

    def _wait_for_disk_low(self, directory, sleep_sec=5):
        """å½“ç›®å½•å¤§å°è¶…è¿‡é«˜æ°´ä½æ—¶é˜»å¡ï¼Œç›´åˆ°é™åˆ°ä½æ°´ä½ä»¥ä¸‹ã€‚

        è¯¥æ–¹æ³•åœ¨ä¸‹è½½å¾ªç¯ä¸­è¢«è°ƒç”¨ä»¥å®ç°ç®€å•çš„å›å‹ï¼Œé¿å…æ— é™åˆ¶æ‹‰å–å¯¼è‡´ç£ç›˜è€—å°½ã€‚
        """
        try:
            high = float(os.getenv('DOWNLOAD_HIGH_WATERMARK_GB', str(self._download_high_watermark_gb)))
            low = float(os.getenv('DOWNLOAD_LOW_WATERMARK_GB', str(self._download_low_watermark_gb)))
        except Exception:
            high = self._download_high_watermark_gb
            low = self._download_low_watermark_gb

        # å¿«é€Ÿåˆ¤æ–­ï¼šå¦‚æœç›®å½•ä¸å­˜åœ¨æˆ–å¤§å°å°äºé«˜æ°´ä½ï¼Œç«‹å³è¿”å›
        try:
            current = self._get_dir_size_gb(directory)
        except Exception:
            return

        while current >= high:
            try:
                logger.warning(f"Disk high watermark reached ({current:.2f}GB >= {high}GB). Pausing downloads...")
            except Exception:
                pass
            time.sleep(sleep_sec)
            try:
                current = self._get_dir_size_gb(directory)
            except Exception:
                break
            if current <= low:
                try:
                    logger.info(f"Disk usage dropped to {current:.2f}GB <= low watermark {low}GB, resuming")
                except Exception:
                    pass
                break
    
    def logout(self):
        """ä¿æŒæ¥å£å…¼å®¹æ€§çš„è™šæ‹Ÿç™»å‡º"""
        print(f"âœ… Logout successful: {self.username}")
        return True
    
    def check_status(self):
        """æ£€æŸ¥PACSè¿æ¥çŠ¶æ€"""
        try:
            assoc = self.ae.associate(
                self.pacs_config['PACS_IP'],
                self.pacs_config['PACS_PORT'],
                ae_title=self.pacs_config['CALLED_AET']
            )
            
            if assoc.is_established:
                assoc.release()
                logger.info("PACS connection status: OK")
                return True
            else:
                logger.warning("Unable to connect to PACS")
                return False
        except Exception as e:
            logger.error(f"PACS connection error: {e}")
            return False
    
    def _query_series_metadata(self, accession_number):
        """æŸ¥è¯¢PACSè·å–Serieså…ƒæ•°æ®"""
        series_metadata = []
        
        try:
            assoc = self.ae.associate(
                self.pacs_config['PACS_IP'],
                self.pacs_config['PACS_PORT'],
                ae_title=self.pacs_config['CALLED_AET']
            )
            
            if not assoc.is_established:
                print("âŒ Cannot build PACS connection")
                return []
            
            try:
                # æŸ¥è¯¢Study
                study_ds = Dataset()
                study_ds.QueryRetrieveLevel = "STUDY"
                study_ds.AccessionNumber = accession_number
                study_ds.StudyInstanceUID = ""
                study_ds.PatientID = ""
                study_ds.PatientName = ""
                study_ds.StudyDate = ""
                
                print(f"ğŸ” Query AccessionNumber: {accession_number}")
                responses = assoc.send_c_find(study_ds, StudyRootQueryRetrieveInformationModelFind)
                
                studies = {}
                for (status, identifier) in responses:
                    if status and status.Status in [0xFF00, 0xFF01]:
                        if identifier and hasattr(identifier, 'StudyInstanceUID'):
                            study_uid = str(identifier.StudyInstanceUID)
                            studies[study_uid] = {
                                'PatientID': str(identifier.PatientID) if hasattr(identifier, 'PatientID') else '',
                                'PatientName': str(identifier.PatientName) if hasattr(identifier, 'PatientName') else '',
                                'StudyDate': str(identifier.StudyDate) if hasattr(identifier, 'StudyDate') else '',
                                'AccessionNumber': accession_number
                            }
                
                if not studies:
                    print(f"âš ï¸  Can't Find AccessionNumber: {accession_number}")
                    return []
                
                # æŸ¥è¯¢æ¯ä¸ªStudyçš„Series
                for study_uid, study_info in studies.items():
                    series_ds = Dataset()
                    series_ds.QueryRetrieveLevel = "SERIES"
                    series_ds.StudyInstanceUID = study_uid
                    series_ds.SeriesInstanceUID = ""
                    series_ds.SeriesNumber = ""
                    series_ds.SeriesDescription = ""
                    series_ds.Modality = ""
                    
                    responses = assoc.send_c_find(series_ds, StudyRootQueryRetrieveInformationModelFind)
                    
                    for (status, identifier) in responses:
                        if status and status.Status in [0xFF00, 0xFF01]:
                            if identifier and hasattr(identifier, 'SeriesInstanceUID'):
                                series_info = dict(study_info)
                                series_info.update({
                                    'StudyInstanceUID': study_uid,
                                    'SeriesInstanceUID': str(identifier.SeriesInstanceUID),
                                    'SeriesNumber': str(identifier.SeriesNumber) if hasattr(identifier, 'SeriesNumber') else '0',
                                    'SeriesDescription': str(identifier.SeriesDescription) if hasattr(identifier, 'SeriesDescription') else 'Unknown',
                                    'Modality': str(identifier.Modality) if hasattr(identifier, 'Modality') else ''
                                })
                                series_metadata.append(series_info)
                
                print(f"ğŸ“Š Find {len(series_metadata)} Series")
                
            finally:
                assoc.release()
                
        except Exception as e:
            print(f"âŒ Query metadata failed: {e}")
        
        return series_metadata
    
    def download_study(self, accession_number, output_dir=".", custom_folder_name=None, on_series_downloaded=None):
        """Download Study data (directly from PACS, no ZIP generation)"""
        print(f"ğŸ” Downloading AccessionNumber: {accession_number}")
        
        # æŸ¥è¯¢Seriesä¿¡æ¯
        series_metadata = self._query_series_metadata(accession_number)
        if not series_metadata:
            print(f"âŒ No data found for: {accession_number}")
            return None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        if custom_folder_name:
            output_path = os.path.join(output_dir, custom_folder_name)
        else:
            output_path = os.path.join(output_dir, f"{accession_number}_{timestamp}")
        
        os.makedirs(output_path, exist_ok=True)
        
        # å­˜å‚¨çŠ¶æ€
        storage_state = {'current_path': '', 'files_received': 0}
        
        def handle_store(event):
            """å¤„ç†C-STOREè¯·æ±‚"""
            try:
                dataset = event.dataset
                dataset.file_meta = event.file_meta
                
                # ä¿å­˜æ–‡ä»¶
                sop_instance_uid = dataset.SOPInstanceUID
                filename = f"{sop_instance_uid}.dcm"
                filepath = os.path.join(storage_state['current_path'], filename)
                
                os.makedirs(storage_state['current_path'], exist_ok=True)
                dataset.save_as(filepath, write_like_original=False)
                
                storage_state['files_received'] += 1
                if storage_state['files_received'] % 10 == 0:
                    print(f"   Received {storage_state['files_received']} files...")
                
                return 0x0000
            except Exception as e:
                print(f"âŒ Failed saving DICOM file: {e}")
                return 0xA700
        
        # å¯åŠ¨C-STORE SCP
        ae_scp = AE(ae_title=self.pacs_config['CALLING_AET'])
        ae_scp.supported_contexts = AllStoragePresentationContexts
        ae_scp.add_requested_context(StudyRootQueryRetrieveInformationModelMove)
        
        server = ae_scp.start_server(
            ('', self.pacs_config['CALLING_PORT']),
            block=False,
            evt_handlers=[(evt.EVT_C_STORE, handle_store)]
        )
        
        try:
            # å»ºç«‹C-MOVEè¿æ¥
            assoc = self.ae.associate(
                self.pacs_config['PACS_IP'],
                self.pacs_config['PACS_PORT'],
                ae_title=self.pacs_config['CALLED_AET']
            )
            
            if not assoc.is_established:
                print("âŒ Unable to establish PACS association")
                return None
            
            try:
                # ä¸‹è½½æ¯ä¸ªSeries
                for i, series in enumerate(series_metadata):
                    series_num = series.get('SeriesNumber', f'Series{i+1}')
                    series_desc = series.get('SeriesDescription', 'Unknown')
                    series_dir = os.path.join(output_path, f"{series_num:0>3}_{self._sanitize_folder_name(series_desc)}")
                    
                    storage_state['current_path'] = series_dir
                    
                    print(f"ğŸ“¥ Downloading series {i+1}/{len(series_metadata)}: {series_num} - {series_desc}")
                    
                    # å½“ç£ç›˜ç©ºé—´è¾¾åˆ°é«˜æ°´ä½æ—¶ï¼Œæš‚åœä¸‹è½½ä»¥ç­‰å¾…è½¬æ¢/æ¸…ç†
                    try:
                        # ä½¿ç”¨ base output dir ä½œä¸ºç£ç›˜æ£€æŸ¥ç›®æ ‡ï¼ˆoutput_dir å‚æ•°ï¼‰
                        self._wait_for_disk_low(output_path)
                    except Exception:
                        pass

                    # å‘é€C-MOVEè¯·æ±‚
                    move_ds = Dataset()
                    move_ds.QueryRetrieveLevel = 'SERIES'
                    move_ds.StudyInstanceUID = series['StudyInstanceUID']
                    move_ds.SeriesInstanceUID = series['SeriesInstanceUID']
                    
                    print(f"   Sending C-MOVE request for Series {series_num}...")
                    
                    # æŠ¥å‘Šä¸‹è½½è¿›åº¦
                    if callable(self.download_progress_callback):
                        try:
                            progress_pct = 40 + int((i / len(series_metadata)) * 40)  # 40-80% ç”¨äºä¸‹è½½
                            self.download_progress_callback(i + 1, len(series_metadata), series_desc, progress_pct)
                        except Exception as cb_e:
                            print(f"   Progress callback error: {cb_e}")
                    
                    responses = assoc.send_c_move(
                        move_ds,
                        self.pacs_config['CALLING_AET'],
                        query_model=StudyRootQueryRetrieveInformationModelMove
                    )
                    
                    # è·Ÿè¸ªC-MOVEå“åº”çŠ¶æ€
                    move_status = None
                    for (status, identifier) in responses:
                        if status:
                            move_status = status.Status
                            if status.Status == 0x0000:
                                print(f"   Series {series_num} C-MOVE completed successfully")
                            elif status.Status != 0xFF00:  # 0xFF00 æ˜¯PendingçŠ¶æ€
                                print(f"   Series {series_num} C-MOVE status: 0x{status.Status:04X}")
                    
                    if move_status is None:
                        print(f"   âš ï¸  Series {series_num}: No C-MOVE response received (timeout or network issue)")
                    
                    time.sleep(0.5)  # çŸ­æš‚å»¶è¿Ÿï¼Œè®©æ–‡ä»¶å†™å…¥å®Œæˆ
                    
                    # é€šçŸ¥å¤–éƒ¨ï¼šè¯¥Seriesä¸‹è½½å®Œæˆ
                    # æ³¨æ„ï¼šå›è°ƒå¿…é¡»åœ¨sleepä¹‹åè°ƒç”¨ï¼Œç¡®ä¿æ–‡ä»¶å·²å®Œå…¨å†™å…¥ç£ç›˜
                    if callable(on_series_downloaded):
                        try:
                            on_series_downloaded(series_dir, series)
                        except Exception as e:
                            print(f"âš ï¸  Series callback failed: {e}")
                
            finally:
                assoc.release()
                
        except Exception as e:
            print(f"âŒ Download error: {e}")
            return None
        finally:
            server.shutdown()
        
        print(f"âœ… Download complete! Received {storage_state['files_received']} files")
        print(f"ğŸ“ Files saved to: {output_path}")
        
        return output_path if storage_state['files_received'] > 0 else None
    
    def extract_zip(self, zip_filepath, extract_dir=None):
        """è§£å‹zip_filepathåˆ°æŒ‡å®šç›®å½•ã€‚

        è‹¥ zip_filepath å·²æ˜¯ç›®å½•ï¼Œåˆ™ç›´æ¥è¿”å›è¯¥ç›®å½•ã€‚
        """
        if not zip_filepath:
            return None
        if os.path.isdir(zip_filepath):
            return zip_filepath
        if not os.path.isfile(zip_filepath):
            return None

        if extract_dir is None:
            base_name = os.path.splitext(os.path.basename(zip_filepath))[0]
            extract_dir = os.path.join(os.path.dirname(zip_filepath), base_name)

        os.makedirs(extract_dir, exist_ok=True)

        try:
            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
        except Exception:
            return None

        return extract_dir

    def _get_required_tag_names(self):
        """è½¬æ¢/é¢„è§ˆæ‰€éœ€çš„æœ€å°DICOMæ ‡ç­¾åˆ—è¡¨ã€‚"""
        return [
            'Modality',
            'WindowCenter',
            'WindowWidth',
            'Rows',
            'Columns',
            'PixelSpacing',
            'ImagerPixelSpacing',
            'PatientOrientation',
            'SpacingBetweenSlices',
            'SliceThickness',
            'PhotometricInterpretation',
            'RescaleSlope',
            'RescaleIntercept',
            'ImageOrientationPatient',
            'ImagePositionPatient'
        ]

    def _normalize_tag_value(self, value):
        if value is None:
            return None
        if hasattr(value, 'value'):
            value = value.value
        if hasattr(value, '__len__') and not isinstance(value, str):
            return [self._normalize_tag_value(v) for v in value]
        try:
            return float(value)
        except (TypeError, ValueError):
            return str(value)

    def _build_sample_tags(self, dcm):
        """ä»æ ·æœ¬DICOMæ„å»ºå¯åºåˆ—åŒ–çš„tagä¿¡æ¯ã€‚"""
        tags = {}
        for tag_name in self._get_required_tag_names():
            try:
                tags[tag_name] = self._normalize_tag_value(getattr(dcm, tag_name, None))
            except Exception:
                tags[tag_name] = None
        return tags

    def _load_sample_tags_from_cache(self, series_dir):
        cache_path = os.path.join(series_dir, "dicom_metadata_cache.json")
        if not os.path.exists(cache_path):
            return None
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            return cache.get('sample_tags')
        except Exception:
            return None

    def _ensure_metadata_cache(self, series_dir, series_name, dicom_files, modality):
        cache_path = os.path.join(series_dir, "dicom_metadata_cache.json")
        if os.path.exists(cache_path):
            return
        self._cache_metadata_for_series(series_dir, series_name, dicom_files, modality)
        if os.path.exists(cache_path):
            return

        sample_tags = None
        try:
            if dicom_files:
                sample_dcm = pydicom.dcmread(dicom_files[0], force=True, stop_before_pixels=True)
                sample_tags = self._build_sample_tags(sample_dcm)
                if not sample_tags.get('Modality'):
                    sample_tags['Modality'] = modality
        except Exception:
            sample_tags = None

        fallback_records = [
            {
                'SeriesFolder': series_name,
                'TotalFilesInSeries': len(dicom_files),
                'FilesReadForMetadata': 0,
                'Modality': modality
            }
        ]
        payload = {
            'modality': modality,
            'records': fallback_records,
            'sample_tags': sample_tags
        }
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        except Exception:
            return

    def _write_minimal_cache(self, series_dir, series_name, modality, sample_dcm=None, file_count=0):
        cache_path = os.path.join(series_dir, "dicom_metadata_cache.json")
        if os.path.exists(cache_path):
            return

        sample_tags = None
        try:
            if sample_dcm is not None:
                sample_tags = self._build_sample_tags(sample_dcm)
                if not sample_tags.get('Modality'):
                    sample_tags['Modality'] = modality
        except Exception:
            sample_tags = None

        payload = {
            'modality': modality,
            'records': [
                {
                    'SeriesFolder': series_name,
                    'TotalFilesInSeries': file_count,
                    'FilesReadForMetadata': 0,
                    'Modality': modality
                }
            ],
            'sample_tags': sample_tags
        }
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        except Exception:
            return
    
    def _is_dicom_file(self, filepath):
        """åˆ¤æ–­æ˜¯å¦ä¸ºDICOMæ–‡ä»¶"""
        if filepath.endswith("json") or filepath.endswith("csv") or filepath.endswith("txt"):
            return False
        try:
            with open(filepath, 'rb') as f:
                f.seek(128)
                dicm = f.read(4)
                if dicm == b'DICM':
                    return True
            
            pydicom.dcmread(filepath, force=True, stop_before_pixels=True)
            return True
        except:
            return False
    
    def _sanitize_folder_name(self, name):
        """æ¸…ç†æ–‡ä»¶å¤¹åç§°ï¼Œç§»é™¤æˆ–æ›¿æ¢Windowså’Œdcm2niixä¸å…¼å®¹çš„å­—ç¬¦"""
        if not name:
            return "Unknown"
        
        name = str(name)
        
        # 1. æ›¿æ¢Windowséæ³•å­—ç¬¦
        name = re.sub(r'[<>"/\\|?*]', '_', name)
        
        # 2. æ›¿æ¢å¯èƒ½å¯¼è‡´dcm2niixé—®é¢˜çš„å­—ç¬¦ç»„åˆ
        # ç‚¹+ç©ºæ ¼ï¼ˆå¦‚ "303. X Elbow" -> "303_X Elbow"ï¼‰
        name = re.sub(r'\.\s+', '_', name)
        # å¤šä¸ªè¿ç»­ç©ºæ ¼è½¬ä¸ºå•ä¸ªä¸‹åˆ’çº¿
        name = re.sub(r'\s+', '_', name)
        # å¤šä¸ªè¿ç»­ç‚¹è½¬ä¸ºå•ä¸ª
        name = re.sub(r'\.+', '.', name)
        
        # 3. ç§»é™¤é¦–å°¾çš„ç‰¹æ®Šå­—ç¬¦
        name = name.strip('. _')
        
        # 4. é•¿åº¦é™åˆ¶
        if len(name) > 50:
            name = name[:50]
        
        # 5. ç¡®ä¿ä¸ä»¥ç‚¹å¼€å¤´æˆ–ç»“å°¾ï¼ˆWindowsé—®é¢˜ï¼‰
        name = name.strip('.')
        
        return name if name else "Unknown"
    
    def organize_dicom_files(self, extract_dir, organized_dir=None, output_format='nifti'):
        """æŒ‰Seriesæ•´ç†DICOMæ–‡ä»¶å¹¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼ (nifti æˆ– npz)"""
        return organize_dicom_files_impl(self, extract_dir, organized_dir, output_format)

    def _process_single_series(self, series_path, series_folder, organized_dir, output_format='nifti'):
        """å¤„ç†å•ä¸ªSeriesç›®å½•ï¼šç»Ÿè®¡ã€è½¬æ¢å¹¶ç§»åŠ¨åˆ° organized_dirã€‚"""
        return process_single_series_impl(self, series_path, series_folder, organized_dir, output_format)
    
    def convert_dicom_to_nifti(self, series_dir, series_name):
        """å°†DICOMåºåˆ—è½¬æ¢ä¸ºNIfTIæ ¼å¼"""
        return convert_dicom_to_nifti_impl(self, series_dir, series_name)
    
    def _convert_to_npz(self, series_dir, series_name):
        """å°†DICOMåºåˆ—è½¬æ¢ä¸ºNPZæ ¼å¼ï¼Œå¹¶æŒ‰ç…§è¦æ±‚è§„èŒƒåŒ–æ–¹å‘"""
        return convert_to_npz_impl(self, series_dir, series_name)

    def _normalize_and_save_npz(self, nii_path, npz_path):
        """åŠ è½½NIfTIï¼Œåˆ©ç”¨DICOMæ–¹å‘ä¿¡æ¯è§„èŒƒåŒ–å¹¶ä¿å­˜ä¸ºNPZ"""
        return normalize_and_save_npz_impl(nii_path, npz_path)

    def _cache_metadata_for_series(self, series_dir, series_name, dicom_files, modality):
        """ç¼“å­˜DICOMå…ƒæ•°æ®ï¼Œé¿å…åˆ é™¤åæ— æ³•æå–æ ‡ç­¾"""
        try:
            if not dicom_files:
                return

            read_all = modality in ['DR', 'MG', 'DX']
            records = self._collect_metadata_from_dicoms(
                dicom_files=dicom_files,
                series_folder=series_name,
                modality=modality,
                read_all=read_all
            )
            if not records:
                records = [
                    {
                        'SeriesFolder': series_name,
                        'TotalFilesInSeries': len(dicom_files),
                        'FilesReadForMetadata': 0,
                        'Modality': modality
                    }
                ]

            sample_tags = None
            try:
                sample_dcm = pydicom.dcmread(dicom_files[0], force=True)
                sample_tags = self._build_sample_tags(sample_dcm)
                if not sample_tags.get('Modality'):
                    sample_tags['Modality'] = modality
            except Exception:
                sample_tags = None

            cache_path = os.path.join(series_dir, "dicom_metadata_cache.json")
            payload = {
                "modality": modality,
                "records": records,
                "sample_tags": sample_tags
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        except Exception:
            return

    def _collect_metadata_from_dicoms(self, dicom_files, series_folder, modality, read_all):
        """ä»DICOMæ–‡ä»¶æå–å…ƒæ•°æ®ï¼ˆä¸å«è´¨æ§å­—æ®µï¼‰"""
        records = []
        try:
            if not dicom_files:
                return records

            current_keywords = self.get_keywords(modality)

            if read_all:
                for idx, dicom_file in enumerate(dicom_files):
                    try:
                        dcm = pydicom.dcmread(dicom_file, force=True, stop_before_pixels=True)
                        metadata = {
                            'SeriesFolder': series_folder,
                            'FileName': os.path.basename(dicom_file),
                            'FileIndex': idx + 1,
                            'TotalFilesInSeries': len(dicom_files)
                        }
                        for keyword in current_keywords:
                            try:
                                value = getattr(dcm, keyword, None)
                                if value is not None:
                                    if hasattr(value, '__len__') and not isinstance(value, str):
                                        if len(value) == 1:
                                            value = value[0]
                                        else:
                                            value = str(value)
                                    elif hasattr(value, 'value'):
                                        value = value.value
                                    metadata[keyword] = str(value)
                                else:
                                    metadata[keyword] = ""
                            except Exception:
                                metadata[keyword] = ""
                        # ç¡®ä¿è®°å½•æ¯å¼  2D å›¾åƒçš„ Rows/Columns ä¿¡æ¯ï¼Œä¾›åç»­é¢„è§ˆæ–¹å‘æ ¡æ­£ä½¿ç”¨
                        try:
                            metadata['Rows'] = str(getattr(dcm, 'Rows', '') or '')
                            metadata['Columns'] = str(getattr(dcm, 'Columns', '') or '')
                        except Exception:
                            metadata['Rows'] = metadata.get('Rows', '')
                            metadata['Columns'] = metadata.get('Columns', '')
                        records.append(metadata)
                    except Exception:
                        continue
            else:
                sample_file = dicom_files[0]
                dcm = pydicom.dcmread(sample_file, force=True)
                metadata = {
                    'SeriesFolder': series_folder,
                    'SampleFileName': os.path.basename(sample_file),
                    'TotalFilesInSeries': len(dicom_files),
                    'FilesReadForMetadata': 1
                }
                for keyword in current_keywords:
                    try:
                        value = getattr(dcm, keyword, None)
                        if value is not None:
                            if hasattr(value, '__len__') and not isinstance(value, str):
                                if len(value) == 1:
                                    value = value[0]
                                else:
                                    value = str(value)
                            elif hasattr(value, 'value'):
                                value = value.value
                            metadata[keyword] = str(value)
                        else:
                            metadata[keyword] = ""
                    except Exception:
                        metadata[keyword] = ""
                try:
                    metadata['Rows'] = str(getattr(dcm, 'Rows', '') or '')
                    metadata['Columns'] = str(getattr(dcm, 'Columns', '') or '')
                except Exception:
                    metadata['Rows'] = metadata.get('Rows', '')
                    metadata['Columns'] = metadata.get('Columns', '')
                records.append(metadata)
        except Exception:
            return []
        return records

    def _build_metadata_record_from_sample(self, series_folder, sample_dcm, total_files, modality):
        """ç”¨æ ·æœ¬DICOMæ„å»ºå•æ¡å…ƒæ•°æ®è®°å½•ã€‚"""
        metadata = {
            'SeriesFolder': series_folder,
            'SampleFileName': getattr(sample_dcm, 'filename', '') or '',
            'TotalFilesInSeries': total_files,
            'FilesReadForMetadata': 1,
            'Modality': modality
        }
        current_keywords = self.get_keywords(modality)
        for keyword in current_keywords:
            try:
                value = getattr(sample_dcm, keyword, None)
                if value is not None:
                    if hasattr(value, '__len__') and not isinstance(value, str):
                        if len(value) == 1:
                            value = value[0]
                        else:
                            value = str(value)
                    elif hasattr(value, 'value'):
                        value = value.value
                    metadata[keyword] = str(value)
                else:
                    metadata[keyword] = ""
            except Exception:
                metadata[keyword] = ""
        return metadata

    def _get_series_sample_dicom(self, series_dir):
        """è¯»å–åºåˆ—ä¸­çš„æ ·æœ¬DICOMç”¨äºæ ‡ç­¾ä¿¡æ¯"""
        try:
            sample_tags = self._load_sample_tags_from_cache(series_dir)
            if isinstance(sample_tags, dict):
                modality = str(sample_tags.get('Modality') or '')
                sample_dcm = SimpleNamespace(**sample_tags)
                return sample_dcm, modality

            dicom_files = []
            for file in os.listdir(series_dir):
                filepath = os.path.join(series_dir, file)
                if os.path.isfile(filepath) and self._is_dicom_file(filepath):
                    dicom_files.append(filepath)
            if not dicom_files:
                return None, ''
            dicom_files.sort()
            dcm = pydicom.dcmread(dicom_files[0], force=True)
            modality = getattr(dcm, 'Modality', '')
            return dcm, modality
        except Exception:
            return None, ''

    def _get_window_params(self, dcm):
        """è·å–çª—å®½çª—ä½"""
        return get_window_params_impl(dcm)

    def _apply_windowing(self, image_2d, dcm):
        """åº”ç”¨çª—å®½çª—ä½å¹¶å½’ä¸€åŒ–åˆ°0-255"""
        return apply_windowing_impl(image_2d, dcm)

    def _resize_with_aspect(self, img, aspect_ratio):
        """æ ¹æ®åƒç´ é—´è·è°ƒæ•´çºµæ¨ªæ¯”"""
        return resize_with_aspect_impl(img, aspect_ratio)

    def _normalize_2d_preview(self, img, target_size=896):
        """2Då›¾åƒæ ‡å‡†åŒ–åˆ°å›ºå®šå¤§å°çš„æ–¹å½¢ç”»å¸ƒ"""
        return normalize_2d_preview_impl(img, target_size=target_size)

    def _generate_series_preview(self, series_dir, series_name, conversion_result, sample_dcm, modality):
        """ä¸ºåºåˆ—ç”ŸæˆPNGé¢„è§ˆå›¾"""
        return generate_series_preview_impl(
            series_dir,
            series_name,
            conversion_result,
            sample_dcm,
            modality,
            self._sanitize_folder_name
        )
    
    def _convert_with_dcm2niix(self, series_dir, series_name):
        """ä½¿ç”¨dcm2niixå·¥å…·è½¬æ¢"""
        return convert_with_dcm2niix_impl(self, series_dir, series_name)

    def _apply_rescale(self, pixel_data, dcm):
        """åº”ç”¨Rescale Slope/Intercept"""
        return apply_rescale_impl(pixel_data, dcm)

    def _apply_photometric(self, pixel_data, dcm):
        """å¤„ç†Photometric Interpretation (MONOCHROME1/2)"""
        return apply_photometric_impl(pixel_data, dcm)

    def _build_affine_from_dicom(self, dcm, slice_spacing=1.0, slice_cosines=None):
        """åŸºäºDICOMæ–¹å‘ä¿¡æ¯æ„å»ºNIfTIä»¿å°„çŸ©é˜µ (RAS)"""
        return build_affine_from_dicom_impl(dcm, slice_spacing=slice_spacing, slice_cosines=slice_cosines)

    def _assess_image_quality(self, dcm):
        """åŸºäºç›´æ–¹å›¾/å¯¹æ¯”åº¦çš„ç®€å•è´¨æ£€ï¼Œè¿”å›0/1"""
        return assess_image_quality_impl(dcm)

    def _assess_image_quality_from_array(self, pixel_data):
        """åŸºäºç›´æ–¹å›¾/å¯¹æ¯”åº¦çš„ç®€å•è´¨æ£€ï¼Œè¿”å›0/1ï¼ˆè¾“å…¥ä¸ºæ•°ç»„ï¼‰"""
        return assess_image_quality_from_array_impl(pixel_data)

    def _assess_converted_file_quality(self, filepath):
        """åŸºäºè½¬æ¢åçš„NPZ/NIfTIæ–‡ä»¶åšè´¨æ£€ï¼Œè¿”å›0/1"""
        return assess_converted_file_quality_impl(filepath)

    def _assess_series_quality_converted(self, converted_files):
        """å¯¹è½¬æ¢åçš„åºåˆ—åšQCï¼Œ<=200å…¨é‡ï¼Œ>200ä¸­é—´Â±3æŠ½æ ·"""
        return assess_series_quality_converted_impl(converted_files)

    def _get_converted_files(self, series_path):
        """è·å–è½¬æ¢åçš„NPZ/NIfTIæ–‡ä»¶åˆ—è¡¨ï¼Œä¼˜å…ˆNPZ"""
        try:
            npz_files = sorted([f for f in os.listdir(series_path) if f.endswith('.npz')])
            if npz_files:
                return [os.path.join(series_path, f) for f in npz_files], 'npz'

            nifti_files = sorted([f for f in os.listdir(series_path) if f.endswith(('.nii.gz', '.nii'))])
            if nifti_files:
                return [os.path.join(series_path, f) for f in nifti_files], 'nifti'

            return [], None
        except Exception:
            return [], None

    def _assess_series_quality(self, dicom_files):
        """å¯¹åºåˆ—åšQCï¼Œ<=200å…¨é‡ï¼Œ>200ä¸­é—´Â±3æŠ½æ ·"""
        return assess_series_quality_impl(dicom_files, pydicom.dcmread)


    def _convert_with_python_libs(self, series_dir, series_name):
        """ä½¿ç”¨Pythonåº“è½¬æ¢DICOMåˆ°NIfTI"""
        return convert_with_python_libs_impl(self, series_dir, series_name)
    
    def extract_dicom_metadata(self, organized_dir, output_excel=None):
        return extract_dicom_metadata_impl(
            organized_dir=organized_dir,
            output_excel=output_excel,
            get_keywords=self.get_keywords,
            get_converted_files=self._get_converted_files,
            assess_converted_file_quality=self._assess_converted_file_quality,
            assess_series_quality_converted=self._assess_series_quality_converted,
            append_mr_cleaned_sheet=self._append_mr_cleaned_sheet
        )

    def _append_mr_cleaned_sheet(self, df: pd.DataFrame, output_excel: str) -> None:
        """å¯¹ MR è®°å½•åšæ²»ç†/è§„èŒƒåŒ–ï¼Œå¹¶å†™å›åˆ°åŒä¸€ä¸ª Excel çš„ MR_Cleaned sheetã€‚"""
        try:
            if df is None or df.empty or 'Modality' not in df.columns:
                return

            mr_df = df[df['Modality'].astype(str).str.upper() == 'MR'].copy()
            if mr_df.empty:
                return

            print(f"\nğŸ”¬ MR_clean: processing {len(mr_df)} MR records...")

            from src.core.mr_clean import process_mri_dataframe

            # forward optional progress callback
            try:
                cleaned_df = process_mri_dataframe(mr_df, progress_callback=self.progress_callback)
            except TypeError:
                # fallback for older MR_clean signature
                cleaned_df = process_mri_dataframe(mr_df)

            with pd.ExcelWriter(
                output_excel,
                engine='openpyxl',
                mode='a',
                if_sheet_exists='replace',
            ) as writer:
                cleaned_df.to_excel(writer, sheet_name='MR_Cleaned', index=False)

            print("âœ… MR_clean: MR_Cleaned sheet written.")
        except Exception as e:
            print(f"âš ï¸  MR_clean skipped/failed: {e}")

    def process_upload_workflow(self, zip_path, base_output_dir, options=None):
        """ä¸Šä¼ ZIPæµç¨‹ï¼šextract -> organize -> convert -> metadataã€‚"""
        options = options or {}

        os.makedirs(base_output_dir, exist_ok=True)

        extract_dir = self.extract_zip(zip_path, os.path.join(base_output_dir, 'extracted'))
        if not extract_dir:
            return {
                'success': False,
                'error': 'Failed to extract zip',
                'extract_dir': None
            }

        organized_dir = extract_dir
        series_info = {}
        excel_file = None

        if options.get('auto_organize', True):
            organized_dir, series_info = self.organize_dicom_files(
                extract_dir,
                output_format=options.get('output_format', 'nifti')
            )

        if options.get('auto_metadata', True):
            excel_file = self.extract_dicom_metadata(organized_dir)

        return {
            'success': True,
            'extract_dir': extract_dir,
            'organized_dir': organized_dir,
            'excel_file': excel_file,
            'series_info': series_info,
            'series_count': len(series_info)
        }
    
    def process_complete_workflow(self, accession_number, base_output_dir="./downloads",
                                auto_extract=True, auto_organize=True, auto_metadata=True,
                                keep_zip=True, keep_extracted=False, output_format='nifti',
                                parallel_pipeline=True):
        """å®Œæ•´çš„å·¥ä½œæµç¨‹ï¼šä¸‹è½½ -> æ•´ç† -> è½¬æ¢ -> æå–å…ƒæ•°æ®"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ Starting full DICOM processing workflow")
        print(f"ğŸ“‹ AccessionNumber: {accession_number}")
        print(f"{'='*80}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(base_output_dir, exist_ok=True)
        
        # æ­¥éª¤1: ä¸‹è½½DICOMæ–‡ä»¶
        print(f"\nğŸ“¥ Step 1: Download DICOM files")

        download_dir_holder = {'path': None}
        # allow configuring pending-series limit to apply backpressure when conversion is slow
        try:
            max_pending = int(os.getenv('MAX_PENDING_SERIES', '4'))
            if max_pending <= 0:
                max_pending = 4
        except Exception:
            max_pending = 4
        series_queue = Queue(maxsize=max_pending)
        series_info = {}
        series_lock = threading.Lock()
        download_done = threading.Event()

        def _on_series_downloaded(series_dir, series_meta):
            series_folder = os.path.basename(series_dir)
            series_queue.put((series_dir, series_folder))

        def _download_worker():
            try:
                download_path = self.download_study(
                    accession_number,
                    base_output_dir,
                    on_series_downloaded=_on_series_downloaded
                )
                download_dir_holder['path'] = download_path
            finally:
                download_done.set()

        # organize worker: multiple workers supported to convert concurrently
        try:
            num_converters = int(os.getenv('NUM_CONVERTERS', '2'))
            if num_converters <= 0:
                num_converters = 2
        except Exception:
            num_converters = 2

        def _organize_worker(organized_dir_local, fmt):
            while True:
                item = series_queue.get()
                if item is None:
                    series_queue.task_done()
                    break
                series_dir, series_folder = item
                try:
                    info = self._process_single_series(series_dir, series_folder, organized_dir_local, fmt)
                    if info:
                        with series_lock:
                            series_info[series_folder] = info
                except Exception as e:
                    print(f"âš ï¸  Series organize failed: {series_folder}: {e}")
                finally:
                    series_queue.task_done()

        if parallel_pipeline and auto_organize:
            organized_dir = os.path.join(base_output_dir, f"{accession_number}_organized")
            os.makedirs(organized_dir, exist_ok=True)

            download_thread = threading.Thread(target=_download_worker, daemon=True)
            # spawn multiple organizers
            organizer_threads = []
            for _ in range(num_converters):
                t = threading.Thread(target=_organize_worker, args=(organized_dir, output_format), daemon=True)
                t.start()
                organizer_threads.append(t)

            download_thread.start()

            # ç­‰å¾…ä¸‹è½½å®Œæˆ
            download_thread.join()
            # é€šçŸ¥æ•´ç†çº¿ç¨‹é€€å‡ºï¼ˆæ”¾å…¥ä¸ worker æ•°ç›¸åŒçš„å“¨å…µï¼‰
            for _ in range(len(organizer_threads)):
                series_queue.put(None)
            series_queue.join()
            for t in organizer_threads:
                t.join()

            download_dir = download_dir_holder['path']
            if not download_dir:
                print("âŒ Download failed, workflow terminated")
                return None
        else:
            download_dir = self.download_study(accession_number, base_output_dir)
            if not download_dir:
                print("âŒ Download failed, workflow terminated")
                return None
        
        results = {
            'accession_number': accession_number,
            'zip_file': download_dir,  # ä¿æŒæ¥å£å…¼å®¹æ€§
            'extract_dir': download_dir,  # ä¿æŒæ¥å£å…¼å®¹æ€§
            'success': False
        }
        
        if auto_organize:
            # æ­¥éª¤2: æ•´ç†DICOMæ–‡ä»¶
            print(f"\nğŸ“ Step 2: Organize DICOM files by series (format: {output_format})")
            if parallel_pipeline:
                # ä½¿ç”¨æµæ°´çº¿æ•´ç†ç»“æœ
                organized_dir = os.path.join(base_output_dir, f"{accession_number}_organized")
                results['organized_dir'] = organized_dir
                results['series_info'] = series_info
            else:
                organized_dir, series_info = self.organize_dicom_files(download_dir, output_format=output_format)
                if not organized_dir:
                    print("âŒ File organization failed, workflow terminated")
                    return results
                results['organized_dir'] = organized_dir
                results['series_info'] = series_info

            if auto_metadata:
                # æ­¥éª¤3: æå–å…ƒæ•°æ® (ç‹¬ç«‹çº¿ç¨‹)
                print(f"\nğŸ“Š Step 3: Extract DICOM metadata")
                excel_name = f"dicom_metadata_{accession_number}.xlsx"
                excel_path = os.path.join(os.path.dirname(organized_dir), excel_name)

                excel_holder = {'path': None}

                def _metadata_worker():
                    excel_holder['path'] = self.extract_dicom_metadata(organized_dir, output_excel=excel_path)

                metadata_thread = threading.Thread(target=_metadata_worker, daemon=True)
                metadata_thread.start()
                metadata_thread.join()

                excel_file = excel_holder['path']
                if excel_file:
                    results['excel_file'] = excel_file
                    results['success'] = True
                else:
                    print("âš ï¸  Metadata extraction failed, previous steps completed")
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"\n{'='*80}")
        if results['success']:
            print(f"ğŸ‰ Workflow completed!")
            print(f"ğŸ“ Organized directory: {results.get('organized_dir', 'N/A')}")
            print(f"ğŸ“„ Excel file: {results.get('excel_file', 'N/A')}")
            print(f"ğŸ“Š Series count: {len(results.get('series_info', {}))}")
        else:
            print(f"âš ï¸  Workflow partially completed")
        print(f"{'='*80}")
        
        return results


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´å·¥ä½œæµç¨‹"""
    print("ğŸ¥ Unified DICOM download and processing system")
    print("ğŸ“¡ Direct PACS server connection")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = DICOMDownloadClient()
    
    # æ£€æŸ¥PACSçŠ¶æ€
    if not client.check_status():
        print("âŒ PACS unavailable, exiting")
        return
    
    # è™šæ‹Ÿç™»å½•ï¼ˆä¿æŒæ¥å£å…¼å®¹æ€§ï¼‰
    client.login("admin", "admin123")
    
    try:
        # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
        accession_number = "Z25043000836"  # ç¤ºä¾‹AccessionNumber
        
        results = client.process_complete_workflow(
            accession_number=accession_number,
            base_output_dir="./dicom_processed",
            auto_extract=True,  # ä¿æŒå…¼å®¹æ€§å‚æ•°
            auto_organize=True,
            auto_metadata=True,
            keep_zip=False,     # ä¿æŒå…¼å®¹æ€§å‚æ•°
            keep_extracted=False,
            output_format='nifti'  # å¯é€‰ 'nifti' æˆ– 'npz'
        )
        
        if results and results['success']:
            print(f"\nğŸŠ Processing complete! See the following files:")
            if 'excel_file' in results:
                print(f"   ğŸ“„ Metadata Excel: {results['excel_file']}")
            if 'organized_dir' in results:
                print(f"   ğŸ“ Organized directory: {results['organized_dir']}")
        else:
            print(f"\nâŒ Processing not fully successful")
    
    finally:
        # è™šæ‹Ÿç™»å‡º
        client.logout()


if __name__ == "__main__":
    main()
